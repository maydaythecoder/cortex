// Data Processing Example in Cortex
// Demonstrates data loading, preprocessing, and analysis

// Constants
let DATA_PATH :: "data/"
let BATCH_SIZE :: 32
let TRAIN_RATIO :: 0.8
let VAL_RATIO :: 0.1
let TEST_RATIO :: 0.1

// Dataset structure
struct Dataset |
  features: tensor
  labels: tensor
  metadata: dict
^

// Data preprocessing functions
func normalize_data[data: tensor] -> tensor |
  let mean_val := mean[data, axis=0]
  let std_val := std[data, axis=0]
  let epsilon := 1e-8
  
  return[(data - mean_val) / (std_val + epsilon)]
^

func standardize_data[data: tensor] -> tensor |
  let min_val := min[data, axis=0]
  let max_val := max[data, axis=0]
  
  return[(data - min_val) / (max_val - min_val)]
^

func remove_outliers[data: tensor, threshold: float] -> tensor |
  let mean_val := mean[data]
  let std_val := std[data]
  let z_scores := abs[(data - mean_val) / std_val]
  
  let mask := z_scores < threshold
  return[data[mask]]
^

// Data augmentation
func augment_data[data: tensor, labels: tensor] |
  let augmented_data := []
  let augmented_labels := []
  
  for [i in range[0, len[data]]] |
    let sample := data[i]
    let label := labels[i]
    
    // Add original sample
    augmented_data := append[augmented_data, sample]
    augmented_labels := append[augmented_labels, label]
    
    // Add noise
    let noisy_sample := sample + randn[len[sample]] * 0.1
    augmented_data := append[augmented_data, noisy_sample]
    augmented_labels := append[augmented_labels, label]
    
    // Add scaled version
    let scaled_sample := sample * (0.9 + rand[1] * 0.2)  // Scale between 0.9 and 1.1
    augmented_data := append[augmented_data, scaled_sample]
    augmented_labels := append[augmented_labels, label]
  ^
  
  return[augmented_data, augmented_labels]
^

// Data splitting
func split_dataset[dataset: Dataset, train_ratio: float, val_ratio: float, test_ratio: float] |
  let total_size := len[dataset.features]
  let train_size := int[total_size * train_ratio]
  let val_size := int[total_size * val_ratio]
  
  // Shuffle indices
  let indices := arange[0, total_size]
  indices := shuffle[indices]
  
  // Split data
  let train_indices := indices[0:train_size]
  let val_indices := indices[train_size:train_size + val_size]
  let test_indices := indices[train_size + val_size:]
  
  let train_data := Dataset{
    features: dataset.features[train_indices],
    labels: dataset.labels[train_indices],
    metadata: dataset.metadata
  }
  
  let val_data := Dataset{
    features: dataset.features[val_indices],
    labels: dataset.labels[val_indices],
    metadata: dataset.metadata
  }
  
  let test_data := Dataset{
    features: dataset.features[test_indices],
    labels: dataset.labels[test_indices],
    metadata: dataset.metadata
  }
  
  return[train_data, val_data, test_data]
^

// Data loader
func create_data_loader[dataset: Dataset, batch_size: int] |
  let num_batches := len[dataset.features] / batch_size
  let batches := []
  
  for [i in range[0, num_batches]] |
    let start_idx := i * batch_size
    let end_idx := min[start_idx + batch_size, len[dataset.features]]
    
    let batch := {
      "features": dataset.features[start_idx:end_idx],
      "labels": dataset.labels[start_idx:end_idx]
    }
    
    batches := append[batches, batch]
  ^
  
  return[batches]
^

// Feature engineering
func extract_features[raw_data: tensor] -> tensor |
  let features := []
  
  for [sample in raw_data] |
    let feature_vector := []
    
    // Basic statistics
    feature_vector := append[feature_vector, mean[sample]]
    feature_vector := append[feature_vector, std[sample]]
    feature_vector := append[feature_vector, min[sample]]
    feature_vector := append[feature_vector, max[sample]]
    
    // Higher-order moments
    feature_vector := append[feature_vector, skew[sample]]
    feature_vector := append[feature_vector, kurtosis[sample]]
    
    // Frequency domain features (simplified)
    let fft := fft[sample]
    feature_vector := append[feature_vector, mean[abs[fft]]]
    feature_vector := append[feature_vector, std[abs[fft]]]
    
    features := append[features, feature_vector]
  ^
  
  return[tensor[features]]
^

// Data validation
func validate_dataset[dataset: Dataset] -> bool |
  // Check for missing values
  let has_nan := any[isnan[dataset.features]]
  if [has_nan] |
    print["Warning: Dataset contains NaN values"]
    return[false]
  ^
  
  // Check for infinite values
  let has_inf := any[isinf[dataset.features]]
  if [has_inf] |
    print["Warning: Dataset contains infinite values"]
    return[false]
  ^
  
  // Check data consistency
  if [len[dataset.features] != len[dataset.labels]] |
    print["Error: Features and labels have different lengths")
    return[false]
  ^
  
  // Check for empty dataset
  if [len[dataset.features] == 0] |
    print["Error: Dataset is empty")
    return[false]
  ^
  
  print["Dataset validation passed")
  return[true]
^

// Data analysis
func analyze_dataset[dataset: Dataset] |
  print["=== Dataset Analysis ==="]
  print["Number of samples: " + str[len[dataset.features]]]
  print["Number of features: " + str[dataset.features.shape[1]])
  print["Number of classes: " + str[len[unique[dataset.labels]]])
  
  // Feature statistics
  let feature_means := mean[dataset.features, axis=0]
  let feature_stds := std[dataset.features, axis=0]
  
  print["Feature means: " + str[feature_means]]
  print["Feature stds: " + str[feature_stds]]
  
  // Class distribution
  let unique_labels := unique[dataset.labels]
  let class_counts := []
  
  for [label in unique_labels] |
    let count := sum[dataset.labels == label]
    class_counts := append[class_counts, count]
    print["Class " + str[label] + ": " + str[count] + " samples")
  ^
  
  // Check for class imbalance
  let max_count := max[class_counts]
  let min_count := min[class_counts]
  let imbalance_ratio := max_count / min_count
  
  if [imbalance_ratio > 2.0] |
    print["Warning: Significant class imbalance detected (ratio: " + str[imbalance_ratio] + ")")
  ^
^
^

// Main data processing pipeline
func process_data[raw_data: tensor, raw_labels: tensor] |
  print["=== Data Processing Pipeline ==="]
  
  // Create initial dataset
  let dataset := Dataset{
    features: raw_data,
    labels: raw_labels,
    metadata: {
      "original_size": len[raw_data],
      "feature_dim": raw_data.shape[1],
      "num_classes": len[unique[raw_labels]]
    }
  }
  
  // Validate dataset
  if [not validate_dataset[dataset]] |
    print["Dataset validation failed, stopping pipeline")
    return[null]
  ^
  
  // Analyze dataset
  analyze_dataset[dataset]
  
  // Remove outliers
  print["Removing outliers...")
  let cleaned_features := remove_outliers[dataset.features, 3.0]
  let cleaned_labels := dataset.labels[dataset.features == cleaned_features]
  
  dataset.features := cleaned_features
  dataset.labels := cleaned_labels
  
  print["After outlier removal: " + str[len[dataset.features]] + " samples")
  
  // Normalize features
  print["Normalizing features...")
  dataset.features := normalize_data[dataset.features]
  
  // Augment data
  print["Augmenting data...")
  let augmented_features, augmented_labels := augment_data[dataset.features, dataset.labels]
  dataset.features := augmented_features
  dataset.labels := augmented_labels
  
  print["After augmentation: " + str[len[dataset.features]] + " samples")
  
  // Split dataset
  print["Splitting dataset...")
  let train_data, val_data, test_data := split_dataset[dataset, TRAIN_RATIO, VAL_RATIO, TEST_RATIO]
  
  print["Train set: " + str[len[train_data.features]] + " samples")
  print["Validation set: " + str[len[val_data.features]] + " samples")
  print["Test set: " + str[len[test_data.features]] + " samples")
  
  // Create data loaders
  let train_loader := create_data_loader[train_data, BATCH_SIZE]
  let val_loader := create_data_loader[val_data, BATCH_SIZE]
  let test_loader := create_data_loader[test_data, BATCH_SIZE]
  
  print["Created " + str[len[train_loader]] + " training batches")
  print["Created " + str[len[val_loader]] + " validation batches")
  print["Created " + str[len[test_loader]] + " test batches")
  
  return[train_loader, val_loader, test_loader]
^
^

// Generate synthetic data for demonstration
func generate_synthetic_data[n_samples: int, n_features: int, n_classes: int] |
  let features := randn[n_samples, n_features]
  let labels := randint[n_samples, 0, n_classes]
  
  // Add some structure to the data
  for [i in range[0, n_samples]] |
    let class_id := labels[i]
    features[i] := features[i] + class_id * 0.5  // Add class-specific bias
  ^
  
  return[features, labels]
^

// Main execution
func main[] |
  print["=== Data Processing in Cortex ==="]
  print[""]
  
  // Generate synthetic data
  let raw_features, raw_labels := generate_synthetic_data[1000, 10, 3]
  print["Generated synthetic dataset with " + str[len[raw_features]] + " samples")
  
  // Process the data
  let train_loader, val_loader, test_loader := process_data[raw_features, raw_labels]
  
  if [train_loader != null] |
    print["")
    print["=== Data Processing Complete ===")
    print["Ready for training with processed data loaders")
  ^
^
